<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- <link href="https://gmpg.org/xfn/11" rel="profile"> -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <!-- MathJax -->
  <!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script> -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML'></script>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- jQuery -->
  <script src="https://google.com/jsapi"></script>; <script> google.load("jquery", "1"); </script>

  <!-- TOC -->
  <script src="/public/js/toc.js"></script>



  <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Exploring Optimization Convergence | Donald Pinckney</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Exploring Optimization Convergence" />
<meta name="author" content="Donald Pinckney" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Tinkering with math, CS, and other random stuff." />
<meta property="og:description" content="Tinkering with math, CS, and other random stuff." />
<link rel="canonical" href="https://donaldpinckney.com/books/tensorflow/src/ch2-linreg/2017-12-27-optimization.html" />
<meta property="og:url" content="https://donaldpinckney.com/books/tensorflow/src/ch2-linreg/2017-12-27-optimization.html" />
<meta property="og:site_name" content="Donald Pinckney" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-12-27T00:00:00-08:00" />
<script type="application/ld+json">
{"description":"Tinkering with math, CS, and other random stuff.","author":{"@type":"Person","name":"Donald Pinckney"},"@type":"BlogPosting","url":"https://donaldpinckney.com/books/tensorflow/src/ch2-linreg/2017-12-27-optimization.html","headline":"Exploring Optimization Convergence","dateModified":"2017-12-27T00:00:00-08:00","datePublished":"2017-12-27T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://donaldpinckney.com/books/tensorflow/src/ch2-linreg/2017-12-27-optimization.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body class="theme-base-0b">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Donald Pinckney
        </a>
      </h1>
      <p class="lead">Tinkering with math, CS, and other random stuff.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="horizontal-block">
        <div class="horizontal-item">
        </div>
        
        <a class="horizontal-item sidebar-nav-item" href="/">Home</a>
        

        
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
        
          
            
              <a class="horizontal-item sidebar-nav-item" href="/categories/">Categories</a>
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
              <a class="horizontal-item sidebar-nav-item" href="/ml.html">ML Resources</a>
            
          
        
          
        
          
        
          
        

        <a class="horizontal-item sidebar-nav-item" href="/books/tensorflow/book">TensorFlow Book</a>

        <div class="horizontal-item">
        </div>
      </div>

      <hr />

      <div class="horizontal-block">
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
            
              <a class="horizontal-item sidebar-nav-item" href="/about.html">About Me</a>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        


        <a href="https://github.com/donald-pinckney" class="sidebar-nav-item horizontal-item" target="_blank">
          <i class="fa fa-github"></i><span class="only-desktop"> donald-pinckney</span>
        </a>
        <a href="https://twitter.com/donald_pinckney" class="sidebar-nav-item horizontal-item" target="_blank">
          <i class="fa fa-twitter"></i><span class="only-desktop">@donald_pinckney</span>
        </a>
      </div>
    </nav>

    <div class="sidebar-footnote">
      <p class="sidebar-footnote">
        &copy; 2018 Donald Pinckney. All rights reserved.
        <!-- <br />
        The views and opinions expressed here are my own, and do not reflect the views of Apple, Inc. -->
      </p>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">

  
    <h1 class="post-title titular floating-box-left">Exploring Optimization Convergence</h1>
  
  
  
    <div class="edit-me floating-box-right">
      
        <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/2017-12-27-optimization.md" target="_blank">
      
      <i class="fa fa-pencil"></i>Edit this page</a>

    </div> 
  

  <span class="post-date">27 Dec 2017

  <div class="post-categories">
  
  
  <a href="/books/tensorflow/book">Read this post via the TensorFlow book instead</a>
  
  
</div>

</span>


<div id="toc"></div>
<script type="text/javascript">
  $(document).ready(function() {
    // alert("READY");
      $('#toc').toc();
  });
</script>


<h1 id="exploring-optimization-convergence">Exploring Optimization Convergence</h1>

<p>In the previous chapter we created a simple single variable linear regression model to fit a data set. While the Python code was actually fairly short and simple, I did gloss over some details related to the optimization, and I hope to use this short chapter to answer some dangling questions about it. Since this chapter doesn’t introduce new models or concepts you can skip it (or come back later) if you prefer. However, getting a feel for optimization is useful for training just about any model, not just the single variable linear regression model, and this chapter should give you insight that is useful for the rest of this book and just about any machine learning you do.</p>

<p>To explore optimization we are going to exactly copy the code from the previous chapter, and experiment with it. To review, let’s look at the part of the code from before that performed the optimization (just look at the previous chapter if you need to review the entire thing):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define optimizer object</span>
<span class="c"># L is what we want to minimize</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c"># Create a session and initialize variables</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c"># Main optimization loop</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">,</span> <span class="n">current_a</span><span class="p">,</span> <span class="n">current_b</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">y_data</span>
    <span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"t = </span><span class="si">%</span><span class="s">g, loss = </span><span class="si">%</span><span class="s">g, a = </span><span class="si">%</span><span class="s">g, b = </span><span class="si">%</span><span class="s">g"</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">,</span> <span class="n">current_a</span><span class="p">,</span> <span class="n">current_b</span><span class="p">))</span>
</code></pre></div></div>

<p>The main unanswered questions to address are:</p>
<ol>
  <li>I said that the learning rate affects how large of steps the optimization algorithm takes in one unit of time. But how do we choose an appropriate value for the learning rate, such as <code class="highlighter-rouge">0.2</code>?</li>
  <li>What is this <code class="highlighter-rouge">AdamOptimizer</code> exactly, are there other choices for optimizers, and how do they differ?</li>
  <li>Currently this code runs for <code class="highlighter-rouge">10000</code> iterations, and that seems good enough to fully optimize <code class="highlighter-rouge">L</code>. But how do we choose this appropriate amount of time for training?</li>
</ol>

<p>There aren’t exact or easy answers to the above questions, but answering these questions is made even harder by the fact that we can not effectively visualize the training progress with the code we have.  Currently we have some <code class="highlighter-rouge">print(...)</code> statements, which is good enough to see that the training error is decreasing, but not much more than that. Let’s start with learning how to visualize training, since this will help us address the other questions and give us a deeper intuition about optimization.</p>

<h2 id="how-to-visualize-training-progress">How to visualize training progress?</h2>

<h3 id="extracting-hyperparameters">Extracting hyperparameters</h3>

<p>One of the simplest ways to visualize training progress is to plot the value of the loss function over time. We could certainly plot the value of the loss function using matplotlib, like we plotted the data set. But TensorFlow actually has a tool built-in for plotting training progress, called TensorBoard. It’s pretty handy, but first we need to do some work to refactor our current code.</p>

<p>The first thing I’d like to do is move <strong>hyperparameters</strong> to the top of the script. What exactly are <strong>hyperparameters</strong>? They are parameters that affect how the training of the model proceeds, but are not part of the model itself. For example, \(a\) and \(b\) are parameters, while the learning rate \(\alpha\) is a hyperparameter. The hyperparameters we have are:</p>
<ol>
  <li>The learning rate, currently <code class="highlighter-rouge">0.2</code>.</li>
  <li>The number of training iterations, currently <code class="highlighter-rouge">10000</code>.</li>
  <li>The choice of optimization routine, currently <code class="highlighter-rouge">tf.train.AdamOptimizer</code> (This isn’t often thought of as a hyperparameter, but here will think of it as one).</li>
</ol>

<p>So, we can just put these into constants at the top of the code. Also, we are going to change these values to give us a simpler starting point:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c">### Hyperparameters ###</span>

<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.000001</span> <span class="c"># Much smaller training rate, to make sure the optimization is at least reliable.</span>
<span class="n">NUM_ITERS</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c"># Twice as many training iterations, just gives us more room to experiment later.</span>
<span class="n">OPTIMIZER_CONSTRUCTOR</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span> <span class="c"># This is the simplest optimization algorithm.</span>

<span class="c"># .. all the rest of the code, hyperparameter constants properly substituted</span>
</code></pre></div></div>

<h3 id="setting-up-tensorboard">Setting up TensorBoard</h3>

<p>Now, we need to add some code to configure TensorBoard for our model. We can setup TensorFlow to automatically keep track of the training progress of <code class="highlighter-rouge">a</code>, <code class="highlighter-rouge">b</code>, and <code class="highlighter-rouge">L</code> by using a <code class="highlighter-rouge">tf.summary.scalar</code>. Since we need to setup summaries for <code class="highlighter-rouge">a</code>, <code class="highlighter-rouge">b</code>, and <code class="highlighter-rouge">L</code>, but before we actually start training, insert this code after <code class="highlighter-rouge">L</code> is defined, but before we start training the model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># ... above this is where L is defined</span>

<span class="c">### Summary setup ###</span>

<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'a'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'L'</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
<span class="n">summary_node</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">merge_all</span><span class="p">()</span>

<span class="n">log_name</span> <span class="o">=</span> <span class="s">"</span><span class="si">%</span><span class="s">g, </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">OPTIMIZER_CONSTRUCTOR</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="s">'/tmp/tensorflow/single_var_reg/'</span> <span class="o">+</span> <span class="n">log_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Open /tmp/tensorflow/single_var_reg/ with tensorboard"</span><span class="p">)</span>

<span class="c"># ... below this is where we create the optimizer and session</span>
</code></pre></div></div>

<p>The first thing this code does is setup <code class="highlighter-rouge">tf.summary.scalar</code> nodes for <code class="highlighter-rouge">a</code>, <code class="highlighter-rouge">b</code>, and <code class="highlighter-rouge">L</code>, and then merge these 3 summaries into a single TensorFlow node, <code class="highlighter-rouge">summary_node</code>.  Later we will need to run <code class="highlighter-rouge">summary_node</code> with our TensorFlow session.</p>

<p>The second thing this code does is setup where TensorFlow will write the logs to. We create a pretty log name by combining <code class="highlighter-rouge">LEARNING_RATE</code> and <code class="highlighter-rouge">OPTIMIZER_CONSTRUCTOR</code> (<code class="highlighter-rouge">__name__</code> gets the name of the Python class), so that we can later compare logs that used different hyperparameters. Then, we create a <code class="highlighter-rouge">tf.summary.FileWriter</code> using the path <code class="highlighter-rouge">/tmp/tensorflow/single_var_reg/</code> concatenated with the log name. This <code class="highlighter-rouge">summary_writer</code> is what we will use to write the <code class="highlighter-rouge">summary_node</code> to log files. Finally we print out a useful message so we remember where the logs are.</p>

<p>The last thing we need to do is make TensorFlow actually write the summary (<code class="highlighter-rouge">summary_node</code>) to the log (using <code class="highlighter-rouge">summary_writer</code>) as training progresses. In our training code before we had a <code class="highlighter-rouge">print</code> statement that showed how training was going, but now we can replace it (or keep it, you choose):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Main optimization loop</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_ITERS</span><span class="p">):</span>
    <span class="c"># We don't need to run L, a, b, just the summary_node.</span>
    <span class="c"># The output of summary_node is stored in summary.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">summary</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">summary_node</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">y_data</span>
    <span class="p">})</span>
    <span class="c"># We write this summary to the log file (print statement was here).</span>
    <span class="n">summary_writer</span><span class="o">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<p>And as a final touch, we can also delete or comment out the code that plots the data set, since it reduces the amount of clicking we have to do when trying different hyperparameters. The complete modified code should look something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c">### Hyperparameters ###</span>

<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.000001</span>
<span class="n">OPTIMIZER_CONSTRUCTOR</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span>
<span class="n">NUM_ITERS</span> <span class="o">=</span> <span class="mi">20000</span>

<span class="c"># Load the data, and convert to 1x30 vectors</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"homicide.csv"</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">num_homicide_deaths</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>


<span class="c">### Model definition ###</span>

<span class="c"># Define x (input data) placeholder</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>

<span class="c"># Define the trainable variables</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"a"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"b"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>

<span class="c"># Define the prediction model</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span>


<span class="c">### Loss function definition ###</span>

<span class="c"># Define y (correct data) placeholder</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>

<span class="c"># Define the loss function</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="c">### Summary setup ###</span>

<span class="n">log_name</span> <span class="o">=</span> <span class="s">"</span><span class="si">%</span><span class="s">g, </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">OPTIMIZER_CONSTRUCTOR</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'a'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'L'</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
<span class="n">summary_node</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">merge_all</span><span class="p">()</span>
<span class="n">summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="s">'/tmp/tensorflow/single_var_reg/'</span> <span class="o">+</span> <span class="n">log_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Open /tmp/tensorflow/single_var_reg/ with tensorboard"</span><span class="p">)</span>


<span class="c">### Training the model ###</span>

<span class="c"># Define optimizer object</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">OPTIMIZER_CONSTRUCTOR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c"># Create a session and initialize variables</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c"># Main optimization loop</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_ITERS</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">summary</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">summary_node</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">y_data</span>
    <span class="p">})</span>
    <span class="n">summary_writer</span><span class="o">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="using-tensorboard">Using TensorBoard</h3>

<p>With the Python code prepared, we can now run it and use TensorBoard to visualize the training. First, run the Python code as usual, using Terminal, or your IDE. This will write logs to <code class="highlighter-rouge">/tmp/tensorflow/single_var_reg/{log_name}</code>. Now, we can open this up with TensorBoard, by running this command in Terminal (make sure to activate the virtual environment first):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ~/tensorflow/bin/tensorboard <span class="nt">--logdir</span><span class="o">=</span>/tmp/tensorflow/single_var_reg/
</code></pre></div></div>
<p>Once you run that command, go to <a href="http://localhost:6006">http://localhost:6006</a> in your web browser, and you should see plots of <code class="highlighter-rouge">L</code>, <code class="highlighter-rouge">a</code>, and <code class="highlighter-rouge">b</code> as the training progressed. With the hyperparameter choices from above, the plots should look like:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/tensorboard_1.png" alt="TensorBoard 1" /></p>

<h2 id="interpreting-the-tensorboard-plots">Interpreting the TensorBoard plots</h2>

<p>Ok, so now we have some plots showing the progression of <code class="highlighter-rouge">L</code>, <code class="highlighter-rouge">a</code>, and <code class="highlighter-rouge">b</code>. What can these plots tell us? Well, with the current choice of <code class="highlighter-rouge">LEARNING_RATE = 0.000001</code>, the plot for <code class="highlighter-rouge">L</code> clearly continues to decrease continually during training. This is good, since this means that the <code class="highlighter-rouge">tf.train.GradientDescentOptimizer</code> is doing it’s job of decreasing the value of the loss function. However, the loss function continues to decrease quickly even towards the end of training: it is reasonable to expect that the loss function would continue to decrease substantially if we continued to train for more iterations. Therefore, we have not found the minimum of the loss function.</p>

<p>To remedy this, we could do one of 3 things: run the training for more iterations, increase the learning rate, or experiment with another optimization algorithm. We don’t want to train for more iterations unless we have to, since that just takes more time, so we will start with increasing the learning rate.</p>

<h2 id="increasing-the-learning-rate">Increasing the learning rate</h2>

<p>Currently the learning rate is <code class="highlighter-rouge">0.000001</code>, and is too small. A pretty easy way to try new learning rates is to go roughly by powers of 10. For now, we can try <code class="highlighter-rouge">0.000001</code>, <code class="highlighter-rouge">0.000005</code>, <code class="highlighter-rouge">0.00001</code>. For each of these, you can just tweak the <code class="highlighter-rouge">LEARNING_RATE</code> constant, and re-run the code. You don’t need to re-run the TensorBoard command (but you can), but make sure to reload the <a href="http://localhost:6006">http://localhost:6006</a> page once you do all the runs. You should get plots that look like:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/tensorboard_2.png" alt="TensorBoard 2" /></p>

<p>We can clearly see the improvement by increasing the learning rate. The final loss obtained with a learning rate of <code class="highlighter-rouge">0.00001</code> is much smaller than our original loss obtained with a learning rate of <code class="highlighter-rouge">0.000001</code>. In addition, we can see that the loss is decreasing more slowly at the end of training. However, the loss function still hasn’t converged, as it is still decreasing significantly. Again, to fix this we could train for longer, but as before we can try increasing the learning rate even more. We can try a learning rate of <code class="highlighter-rouge">0.00005</code>, and we get this:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/tensorboard_3.png" alt="TensorBoard 3" /></p>

<p>Huh? No plot!?!?? If you mouse over the <code class="highlighter-rouge">L</code> plot, TensorBoard will say that the value of <code class="highlighter-rouge">L</code> is NaN (not a number). What gives? Well, if the learning rate is too big then <code class="highlighter-rouge">tf.train.GradientDescentOptimizer</code> can explode: <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code> (and consequently <code class="highlighter-rouge">L</code>) increase to infinity and become NaN. What “too big” is depends on the specific problem.</p>

<p>At this point there isn’t a lot more that we can do by just tweaking the learning rate: it’s either too big and causes the optimization to explode, or is too small to achieve convergence in <code class="highlighter-rouge">20000</code> iterations. We can certainly try more learning rates between <code class="highlighter-rouge">0.00001</code> and <code class="highlighter-rouge">0.00005</code>, but it won’t be a ton better than what we already have.</p>

<h2 id="using-different-optimization-algorithms">Using different optimization algorithms</h2>

<p>So far we have been using <code class="highlighter-rouge">tf.train.GradientDescentOptimizer</code>. It is the simplest, classic way to iteratively train machine learning models. As discussed in the previous chapter, it is like moving a ball downhill, according to the current slope (aka the derivative). Generally, gradient descent is part of a class of optimization algorithms called <strong>first-order methods</strong>, since it uses only information from the first derivative of the loss function, and not higher-order derivatives. First-order methods are currently the dominant way to train most machine learning models.</p>

<p>In fact, there are many first order methods, other than simple gradient descent. Most of them are designed to offer an advantage over other first-order methods via speed to find convergence, reliability, ease of use, etc. For a fairly in-depth exploration, see <a href="http://ruder.io/optimizing-gradient-descent/index.html">this blog post</a>. To see the different optimization algorithms that are built-in to TensorFlow, see <a href="https://www.tensorflow.org/api_guides/python/train#Optimizers">the documentation here</a>. In this list, you can see the <code class="highlighter-rouge">tf.train.AdamOptimizer</code> that we used before, and the classic <code class="highlighter-rouge">tf.train.GradientDescentOptimizer</code>.</p>

<p>We can experiment with any number of these. Here, I’‘ll demonstrate experimenting with <code class="highlighter-rouge">tf.train.AdagradOptimizer</code>, but feel free to play around with any of them. To use <code class="highlighter-rouge">tf.train.AdagradOptimizer</code> we just need to change <code class="highlighter-rouge">OPTIMIZER_CONSTRUCTOR</code>, and set <code class="highlighter-rouge">LEARNING_RATE</code> back to <code class="highlighter-rouge">0.000001</code> for good measure:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.000001</span>
<span class="n">OPTIMIZER_CONSTRUCTOR</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span>
</code></pre></div></div>

<p>Running this we see:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/tensorboard_4.png" alt="TensorBoard 4" /></p>

<p>Well, this is disappointing: <code class="highlighter-rouge">L</code> did not seem to decrease at all during training. The problem is that the learning rate used by gradient descent is really an entirely different learning rate from the Adagrad one: conceptually they are similar, but are on entirely different scales numerically. So, we just need to try different learning rates for Adagrad now. Since the value of <code class="highlighter-rouge">L</code> stayed constant with this very small learning rate, we expect that we need to try much larger learning rates for Adagrad. In fact, by trying learning rates of 0.5, 1, and 5, we get these plots:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/tensorboard_5.png" alt="TensorBoard 5" /></p>

<p>Now this is looking like progress! For the first time we start to get a sense of the loss rapidly decreasing, and then slowing down substantially. In addition, the final value of <code class="highlighter-rouge">a</code> is now negative (which we know is correct) compared to previous runs which ended either positive or close to zero. However, by looking at the plots for <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code> (and to a lesser degree <code class="highlighter-rouge">L</code>) we can see that we still haven’t achieved convergence: <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code> haven’t stopped changing substantially at the end of training. So, time to increase the learning rate even more! Before doing so, I am going to delete the logs of previous runs, except for the Adagrad run with a learning rate of 5, so that we can read the plots more clearly:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This will delete logs of all the runs</span>
rm <span class="nt">-rf</span> /tmp/tensorflow/single_var_reg/
<span class="c"># Or, you can delete a specific run, for example:</span>
rm <span class="nt">-rf</span> /tmp/tensorflow/single_var_reg/5e-05,<span class="se">\ </span>GradientDescentOptimizer/
</code></pre></div></div>

<p>By trying learning rates of 10 and 50, we finally achieve convergence:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/tensorboard_6.png" alt="TensorBoard 6" /></p>

<p>Qualitatively, this looks like convergence (with a learning rate of 10, and certainly with a learning rate of 50) since the progress that Adagrad is making on decreasing <code class="highlighter-rouge">L</code> (and adjusting <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>) has hit a brick wall: no matter how long we run Adagrad, we can’t seem to get a loss function value lower than about \(3.9296 \cdot 10^4 \), and similarly for the values of <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. We’ve finally trained our model completely.</p>

<p>Unfortunately, I don’t know of an easy way to intuitively understand the differences between Adagrad, Adam, and other first-order methods. <a href="http://ruder.io/optimizing-gradient-descent/index.html">This blog post</a> does give some mathematical analysis that explains what each algorithm tries to improve upon, and some reasoning for choosing an algorithm, but it can be tricky to apply to real problems. In general, you can always start with the simplest algorithm (gradient descent), and if it isn’t converging quickly enough for you, then you can switch to a more sophisticated algorithm, such as Adagrad, Adam, or others.</p>

<h1 id="concluding-remarks">Concluding Remarks</h1>

<p>The experimental nature of this chapter should illustrate the practicalities of machine learning: a lot of cutting-edge machine learning currently involves running multiple experiments to try to find the best combination of hyperparameters. There isn’t a golden rule for choosing the optimization algorithm and hyperparameters, but hopefully this chapter demonstrates how to alter the algorithm and hyperparameters in TensorFlow and monitor convergence using TensorBoard. The most important takeaways are:</p>
<ol>
  <li>Learning how to use TensorBoard</li>
  <li>Recognizing convergence</li>
  <li>Recognizing the symptoms of too small of a learning rate</li>
  <li>Recognizing the symptoms of too large of a learning rate</li>
</ol>

<p>In future chapters I won’t include the code specifically for TensorBoard (unless it is important to that chapter) since I don’t want it to get in the way of actual models, but <em>I would highly encourage you to insert your own TensorBoard summary code</em>, and monitor plots of convergence in TensorBoard, since it is useful both educationally and practically.</p>

<h1 id="exercises">Exercises</h1>

<ol>
  <li>Experiment on your own with a few other built-in TensorFlow optimization algorithms, and try different learning rates. If you prefer a more focused goal, try to beat my configuration of an Adagrad optimizer with a learning rate of 50, and converge faster. Also note that some optimization algorithms have additional hyperparameters other than the learning rate. See the TensorFlow documentation for information about these.</li>
  <li>One other cause of slow convergence for the homicide rate linear regression is the somewhat extreme scaling of the problem. The \(y\) variable is a whole order of magnitude greater than the \(x\) variable, and this affects optimization. We will actually look at this problem specifically in chapter 2.4, but for now you can experiment on your own with one solution: instead of using the \(x\) and \(y\) data directly from the data set, modify them first to rescale them. A quick, hacky way is to modify the code that loads the data, so that \(x\) and \(y\) vary between 0 and 1:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Load the data, and convert to 1x30 vectors</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"homicide.csv"</span><span class="p">)</span>
<span class="c"># 21 and 50 are the min and max of x_data</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="mf">21.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">50.0</span> <span class="o">-</span> <span class="mf">21.0</span><span class="p">)</span> 
<span class="c"># 196 and 653 are the min and max of y_data</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">num_homicide_deaths</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="mf">196.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">653.0</span> <span class="o">-</span> <span class="mf">196.0</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>On your own, add this code and see if you can achieve convergence using only gradient descent. You can also see how quickly you can achieve convergence using a more advanced algorithm such as Adam.</p>
  </li>
</ol>

<h1 id="complete-code">Complete Code</h1>

<p>The complete code including TensorBoard summaries, and using Adagrad is <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/single_var_reg_optim.py">available on GitHub</a> and directly below. Note that this code lacks the plotting of the data and the linear regression line:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c">### Hyperparameters ###</span>

<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">50.0</span>
<span class="n">OPTIMIZER_CONSTRUCTOR</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span>
<span class="n">NUM_ITERS</span> <span class="o">=</span> <span class="mi">20000</span>

<span class="c"># Load the data, and convert to 1x30 vectors</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"homicide.csv"</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">num_homicide_deaths</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>


<span class="c">### Model definition ###</span>

<span class="c"># Define x (input data) placeholder</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>

<span class="c"># Define the trainable variables</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"a"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"b"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>

<span class="c"># Define the prediction model</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span>


<span class="c">### Loss function definition ###</span>

<span class="c"># Define y (correct data) placeholder</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>

<span class="c"># Define the loss function</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="c">### Summary setup ###</span>

<span class="n">log_name</span> <span class="o">=</span> <span class="s">"</span><span class="si">%</span><span class="s">g, </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">OPTIMIZER_CONSTRUCTOR</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'a'</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'L'</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
<span class="n">summary_node</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">merge_all</span><span class="p">()</span>
<span class="n">summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="s">'/tmp/tensorflow/single_var_reg/'</span> <span class="o">+</span> <span class="n">log_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Open /tmp/tensorflow/single_var_reg/ with tensorboard"</span><span class="p">)</span>


<span class="c">### Training the model ###</span>

<span class="c"># Define optimizer object</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">OPTIMIZER_CONSTRUCTOR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c"># Create a session and initialize variables</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c"># Main optimization loop</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_ITERS</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">summary</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">summary_node</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">x_data</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">y_data</span>
    <span class="p">})</span>
    <span class="n">summary_writer</span><span class="o">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="c"># print("t = %g, loss = %g, a = %g, b = %g" % (t, current_loss, current_a, current_b))</span>
</code></pre></div></div>


</div>


<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
  </ul>
</div>

    </div>

  </body>
</html>
