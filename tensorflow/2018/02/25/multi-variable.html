<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- <link href="https://gmpg.org/xfn/11" rel="profile"> -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <!-- MathJax -->
  <!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script> -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML'></script>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Multi Variable Linear Regression | Donald Pinckney</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Multi Variable Linear Regression" />
<meta name="author" content="Donald Pinckney" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Multi Variable Regression" />
<meta property="og:description" content="Multi Variable Regression" />
<link rel="canonical" href="https://donaldpinckney.com/tensorflow/2018/02/25/multi-variable.html" />
<meta property="og:url" content="https://donaldpinckney.com/tensorflow/2018/02/25/multi-variable.html" />
<meta property="og:site_name" content="Donald Pinckney" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-25T00:00:00-08:00" />
<script type="application/ld+json">
{"description":"Multi Variable Regression","author":{"@type":"Person","name":"Donald Pinckney"},"@type":"BlogPosting","url":"https://donaldpinckney.com/tensorflow/2018/02/25/multi-variable.html","headline":"Multi Variable Linear Regression","dateModified":"2018-02-25T00:00:00-08:00","datePublished":"2018-02-25T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://donaldpinckney.com/tensorflow/2018/02/25/multi-variable.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body class="theme-base-0b">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Donald Pinckney
        </a>
      </h1>
      <p class="lead">Tinkering with math, CS, and other random stuff.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="horizontal-block">
        <div class="horizontal-item">
        </div>
        
        <a class="horizontal-item sidebar-nav-item" href="/">Home</a>
        

        
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
        
          
            
              <a class="horizontal-item sidebar-nav-item" href="/categories/">Categories</a>
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
              <a class="horizontal-item sidebar-nav-item" href="/ml.html">ML Resources</a>
            
          
        
          
        
          
        
          
        

        <a class="horizontal-item sidebar-nav-item" href="/books/tensorflow/book">TensorFlow Book</a>

        <div class="horizontal-item">
        </div>
      </div>

      <hr />

      <div class="horizontal-block">
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
            
              <a class="horizontal-item sidebar-nav-item" href="/about.html">About Me</a>
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        


        <a href="https://github.com/donald-pinckney" class="sidebar-nav-item horizontal-item" target="_blank">
          <i class="fa fa-github"></i><span class="only-desktop"> donald-pinckney</span>
        </a>
        <a href="https://twitter.com/donald_pinckney" class="sidebar-nav-item horizontal-item" target="_blank">
          <i class="fa fa-twitter"></i><span class="only-desktop">@donald_pinckney</span>
        </a>
      </div>
    </nav>

    <div class="sidebar-footnote">
      <p class="sidebar-footnote">
        &copy; 2018 Donald Pinckney. All rights reserved.
        <!-- <br />
        The views and opinions expressed here are my own, and do not reflect the views of Apple, Inc. -->
      </p>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">

  
    <h1 class="post-title titular floating-box-left">Multi Variable Linear Regression</h1>
  
  
  
    <div class="edit-me floating-box-right">
      
        <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/2017-02-25-multi-variable.md" target="_blank">
      
      <i class="fa fa-pencil"></i>Edit this page</a>

    </div> 
  

  <span class="post-date">25 Feb 2018

  <div class="post-categories">
  
  
  <a href="/books/tensorflow/book">Read the TensorFlow book</a>
  
  
</div>

</span>

  <h1 id="multi-variable-regression">Multi Variable Regression</h1>

<p>In <a href="/books/tensorflow/book/ch2-linreg/2017-12-03-single-variable.html">chapter 2.1</a> we learned the basics of TensorFlow by creating a single variable linear regression model. In this chapter we expand this model to handle multiple variables. Note that less time will be spent explaining the basics of TensorFlow: only new concepts will be explained, so feel free to refer to previous chapters as needed.</p>

<h2 id="motivation">Motivation</h2>

<p>Recall that a single variable linear regression model can learn to predict an output variable \(y\) under these conditions:</p>
<ol>
  <li>There is only one input variable, \(x\)</li>
  <li>There is a linear relationship between \(y\) and \(x\), that is, \(y \approx ax + b\)</li>
</ol>

<p>In practice, the above conditions are very limiting: if you have a simple data set then by all means you should try using single variable linear regression, but in most cases we have significantly more complex data. For example, consider using the following (abbreviated) <a href="https://www.kaggle.com/camnugent/california-housing-prices">data from the 1990 census</a> to learn to predict housing prices. Note that each row represents a single housing district:</p>

<table>
  <thead>
    <tr>
      <th>House Median Age</th>
      <th>Total Rooms</th>
      <th>Total Bedrooms</th>
      <th>…</th>
      <th>Median House Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>41.0</td>
      <td>880.0</td>
      <td>129.0</td>
      <td>…</td>
      <td>452600.0</td>
    </tr>
    <tr>
      <td>21.0</td>
      <td>7099.0</td>
      <td>1106.0</td>
      <td>…</td>
      <td>358500.0</td>
    </tr>
    <tr>
      <td>52.0</td>
      <td>1467.0</td>
      <td>190.0</td>
      <td>…</td>
      <td>352100.0</td>
    </tr>
    <tr>
      <td>52.0</td>
      <td>1274.0</td>
      <td>235.0</td>
      <td>…</td>
      <td>341300.0</td>
    </tr>
    <tr>
      <td>52.0</td>
      <td>1627.0</td>
      <td>280.0</td>
      <td>…</td>
      <td>342200.0</td>
    </tr>
    <tr>
      <td>52.0</td>
      <td>919.0</td>
      <td>213.0</td>
      <td>…</td>
      <td>269700.0</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
  </tbody>
</table>

<p>To predict the values of houses, we have at least 3 real-valued variables (age, number of rooms, number of dedrooms) that could potentially be useful. To analyze this sort of complex, real-world data we need to learn to handle multiple input variables.</p>

<p>One approach to handling multiple variables would be to reduce the number of input variables to only 1 variable, and then training a single variable linear regression model using that. In fact, an important area of research in machine learning (and one that will be covered later) called <strong><a href="https://en.wikipedia.org/wiki/dimensionality_reduction">dimensionality reduction</a></strong> deals with this problem of reducing the number of variables. However, it’s important to realize that the number of variables can only be reduced so far, and its extremely rare that you can reduce a data set to only 1 variable. For now you need to take this statement on faith, but in later chapters we will investigate it more thoroughly.</p>

<p>So, it seems that we will have to deal with training models that can handle multiple variables. In this chapter we learn how to allow multiple input variables in our linear regression model. Such a model is called multi variable linear regression, or just linear regression.</p>

<h2 id="theory">Theory</h2>

<p>Most of the theory is similar to the theory for single variable linear regression, but we will need to augment and generalize it to handle multiple variables.</p>

<h3 id="data-set-format">Data set format</h3>

<p>Previously we defined our data set \(D\) as consisting of many example pairs of \(x\) and \(y\), where \(m\) is the number of examples:
\[
    D = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m))} \}
\]</p>

<p>Note that I have changed the notation compared to before. The notation \(x^{(i)}\) refers to the \(i\)’th \(x\) training example, it does <em>NOT</em> mean \(x\) to the \(i\)’th power, which would be written as \(x^i\). I promise the notation change will be useful shortly.</p>

<p>Alternatively, we can write \(D\) as 2 vectors of shape 1 x \(m\):
\[
    D_x = \begin{bmatrix}
            x^{(1)}, 
            x^{(2)}, 
            \dots, 
            x^{(m)}
    \end{bmatrix} \<br />
    D_y = \begin{bmatrix}
            y^{(1)},
            y^{(2)},
            \dots,
            y^{(m)}
         \end{bmatrix}
\]</p>

<p>But now, we need each \(x^{(i)}\) example to contain multiple numbers, one for each input variable.  Let \(n\) be the number of input variables. Then the easiest way to write this is to let each \(x^{(i)}\) be a vector of shape \(n\) x 1. That is,
\[
    x^{(i)} = \begin{bmatrix}
        x^{(i)}_1 \\ 
        x^{(i)}_2 \<br />
        \vdots \<br />
        x^{(i)}_j \<br />
        \vdots \<br />
        x^{(i)}_n
    \end{bmatrix}
\]
Note that the notation \(x^{(i)}_j\) denotes the \(j\)’th input variable in the \(i\)’th example data.</p>

<p>Since each \(x^{(i)}\) has \(n\) rows, and \(D_x\) has \(m\) columns, each of which is an \(x^{(i)}\), we can write \(D_x\) as a massive \(n \times m\) matrix:
\[
    D_x = \begin{bmatrix}
            x^{(1)}, 
            x^{(2)}, 
            \dots, 
            x^{(m)} \end{bmatrix}
        = \begin{bmatrix}
            x^{(1)}_1 &amp; x^{(2)}_1  &amp; \dots &amp; x^{(i)}_1 &amp; \dots &amp; x^{(m)}_1 \<br />
            x^{(1)}_2 &amp; x^{(2)}_2  &amp; \dots &amp; x^{(i)}_2 &amp; \dots &amp; x^{(m)}_2 \<br />
            \vdots &amp; \vdots  &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \<br />
            x^{(1)}_j &amp; x^{(2)}_j  &amp; \dots &amp; x^{(i)}_j &amp; \dots &amp; x^{(m)}_j \<br />
            \vdots &amp; \vdots  &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \<br />
            x^{(1)}_n &amp; x^{(2)}_n  &amp; \dots &amp; x^{(i)}_n &amp; \dots &amp; x^{(m)}_n \<br />
        \end{bmatrix}
\]
So, each column of \(D_x\) represents a single input data example. We don’t need to change the 1 x \(m\) vector \(D_y\), since we still only have 1 output variable.</p>

<h3 id="model-concept">Model concept</h3>

<p>So, we now have an input data matrix \(D_x\) with each column vector representing a single input data example, and we have the corresponding \(D_y\) row vector, each entry of which is an output data example. How do we define a model which can linearly estimate the output \(y’^{(i)}\) given the input data vector \(x^{(i)}\)? Let’s build it up from simple concepts, and build towards more complex linear algebra.</p>

<p>Since we want \(y’^{(i)}\) to depend linearly on each \(x^{(i)}_j\) for \(1 \leq j \leq n\), we can write:
\[
    y’^{(i)} = a_1 x^{(i)}_1 + a_2 x^{(i)}_2 + \cdots + a_j x^{(i)}_j + \cdots + a_n x^{(i)}_n + b
\]</p>

<p>This is fine mathematically, but it’s not very general. Suppose \(n = 100\): then we would have to literally write out 100 terms in our TensorFlow code. We can generalize this using linear algebra. Let \(A\) be a row vector of shape 1 x \(n\), containing each \(a_j\):
\[
    A = \begin{bmatrix}
            a_1, 
            a_2, 
            \cdots, 
            a_j,
            \cdots,
            a_n
    \end{bmatrix}
\]</p>

<p>Now, let’s see what happens if we compute \(A x^{(i)}\), as matrix multiplication. Note that \(A\) has shape 1 x \(n\) and \(x^{(i)}\) has shape \(n\) x 1. This is perfect! When performing matrix multiplication, the inner dimensions (in this case \(n\) and \(n\)) have to match, and the outer dimensions (in this case \(1\) and \(1\)) determine the output shape of the multiplication. So \(A x^{(i)}\) will have shape 1 x 1, or in other words, just a single number, in fact it is exactly \(y’^{(i)}\). How does this matrix multiplication exactly work? I’ll refer you to <a href="https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro">this video by Khan Academy</a>, and explain it briefly in this case. Here, it is easier since \(A\) is a row vector, and \(x^{(i)}\) is a column vector. We simply multiply each corresponding entry, and add it all up:
\[
    A x^{(i)} + b
    = \begin{bmatrix}
            a_1, 
            a_2, 
            \cdots, 
            a_j,
            \cdots,
            a_n
    \end{bmatrix} \begin{bmatrix}
        x^{(i)}_1 \\ 
        x^{(i)}_2 \<br />
        \vdots \<br />
        x^{(i)}_j \<br />
        \vdots \<br />
        x^{(i)}_n
    \end{bmatrix} + b
    = a_1 x^{(i)}_1 + a_2 x^{(i)}_2 + \cdots + a_j x^{(i)}_j + \cdots + a_n x^{(i)}_n + b
    = y’^{(i)}
\]</p>

<p>This matrix equation, \(y’(x, A, b) = Ax + b\) is exactly what we want as our model. As one final note, recall that in the actual implementation, we don’t want \(x\) and \(y’\) to represent just one input data and predicted output, we want them to represent several. Since \(x\) is a column vector, the natural way to represent multiple input data points is with a matrix, very similar to the matrix \(D_x\), just not necessarily with <em>all</em> the columns of \(D_x\), and \(y’\) should be a row vector. Specifically, \(A\) has shape 1 x \(n\), \(x\) has shape \(n\) x <code class="highlighter-rouge">None</code>, and \(y\) has shape 1 x <code class="highlighter-rouge">None</code>, using the TensorFlow convention that <code class="highlighter-rouge">None</code> represents a yet-to-be-determined matrix size.</p>

<p>Now defining the loss function is pretty much the same as before, just using the new model:
\[
     L(A, b) = \sum_{i=1}^m (y’(x^{(i)}, A, b) - y^{(i)})^2 = \sum_{i=1}^m (A x^{(i)} + b - y^{(i)})^2
\]</p>

<p>To minimize the loss function, we use the same process as before, gradient descent. However, previously the gradient descent was altering 2 variables (\(a\) and \(b\)) so as to minimize the loss function, and so we could plot the loss function and gradient descent progress in terms of \(a\) and \(b\). However, now the optimization needs to alter many more variables, since \(A\) actually contains \(n\) variables, the gradient descent must be performed in \(n+1\) dimensional space, and we don’t have an easy way to visualize this.</p>

<p>With the more general linear algebra formulation of linear regression under our belts, let’s move on to actually coding stuff.</p>

<h2 id="implementation">Implementation</h2>

<p>As before, we need to: import data, define the model, define the loss function, run gradient descent, and finally make predictions. Many steps will be similar to the single variable case, but for completeness I will walk through them briefly.</p>

<p>For building and testing the implementation we will use a synthetic data set consisting of \(n=2\) input variables. You can download <a href="/books/tensorflow/book/ch2-linreg/code/linreg-multi-synthetic-2.csv">the synthetic data set here</a>. By synthetic, I mean that I purposefully created a very nicely behaved data set so that we can practice implementing multi variable linear regression, and verify that we converged to the right answer. In fact, the synthetic data is generated as \(y = 2x_1 + 1.3x_2 + 4 + \varepsilon \) where \(\varepsilon\) is random noise. If we implement multi variable linear regression correctly, then we should obtain approximately \(A = \begin{bmatrix} 2, 1.3 \end{bmatrix}, b = 4\). This plot illustrates what the data looks like in 3 dimensions, essentially a plane in 3 dimensions with some random fluctuations:</p>

<p><img src="/books/tensorflow/book/ch2-linreg/assets/linreg-multi-synthetic-2.png" alt="scatter" /></p>

<h3 id="importing-the-data">Importing the data</h3>

<p>As explained above, the input data set can be organized as an \(n \times m\) matrix. Since we will load the entire data set (input and output) from a single CSV file, and we have 2 input variables, the CSV file will contain 3 columns: the first 2 are the input variables, and the last one is the output variable. So, first we load the CSV file into an \(m\) x 3 matrix, and then separate the first 2 columns from the last:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c"># First we load the entire CSV file into an m x 3</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"linreg-multi-synthetic-2.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c"># We extract all rows and the first 2 columns into X_data</span>
<span class="c"># Then we flip it</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="c"># We extract all rows and the last column into y_data</span>
<span class="c"># Then we flip it</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="c"># And make a convenient variable to remember the number of input columns</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
</code></pre></div></div>

<p>The syntax <code class="highlighter-rouge">D[:, 0:2]</code> might be new, particularly if you haven’t worked with NumPy before. In the single variable implementation we used Panda’s functionality to access the columns by column name. This is a great approach, but sometimes you might need to be more flexible in how you access columns of data.</p>

<blockquote>
  <p><strong>Note:</strong> The basic syntax for subscripting a matrix is: <code class="highlighter-rouge">D[3, 6]</code> (for example), which refers to the row at index 3 and the column at index 6 in the matrix <code class="highlighter-rouge">D</code>. Note that in <code class="highlighter-rouge">numpy</code> the row and column indices start at 0! This means that <code class="highlighter-rouge">D[0, 0]</code> refers to the top-left entry of matrix <code class="highlighter-rouge">D</code>. If you are coming from a pure math background, or have used MATLAB before, it is a common error to assume the indices start at 1. <br /><br />
Now for slicing, the <code class="highlighter-rouge">:</code> character is used to indicate a range. If it is used by itself, it indicates the entire range of rows / columns. For example, <code class="highlighter-rouge">D[:, 42]</code> refers to all rows of <code class="highlighter-rouge">D</code>, and the column at index 42. If it is used with indices, then <code class="highlighter-rouge">i:j</code> indicates the range of rows / columns at indices <code class="highlighter-rouge">i</code>, <code class="highlighter-rouge">i+1</code>, …, <code class="highlighter-rouge">j-1</code>, but <em>not</em> including <code class="highlighter-rouge">j</code>. <br /><br />
So, <code class="highlighter-rouge">D[:, 0:2]</code> means to read the values in <code class="highlighter-rouge">D</code> at all rows and at columns with index <code class="highlighter-rouge">0</code> and <code class="highlighter-rouge">1</code> (the entire first 2 columns, i.e. the input data columns). Likewise, <code class="highlighter-rouge">D[:, 2]</code> means to read the values in <code class="highlighter-rouge">D</code> at all rows and at the column of index <code class="highlighter-rouge">2</code> (the entire last column, i.e. the output data column).</p>
</blockquote>

<p>This matrix subscripting and slicing is almost what we want, but not quite. The problem is that <code class="highlighter-rouge">D[:, 0:2]</code>, which contains our \(D_x\) data, is a matrix of shape \(m \times n\), but earlier we decided that we wanted \(D_x\) to be an \(n \times m\) matrix, so we need to flip it. To do so, we use the <a href="https://en.wikipedia.org/wiki/Transpose"><strong>transpose</strong></a> of the matrix. Mathematically we write the transpose of a matrix \(A\) as \(A^T\), and in Python we can compute it using <code class="highlighter-rouge">A.transpose()</code>. Essentially, the transpose of a matrix simply flips it along the diagonal, as shown in this animation:</p>

<center>
<p><a href="https://commons.wikimedia.org/wiki/File:Matrix_transpose.gif#/media/File:Matrix_transpose.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif" alt="Matrix transpose.gif" /></a><br />By <a href="//commons.wikimedia.org/wiki/User:LucasVB" title="User:LucasVB">LucasVB</a> - <a href="https://commons.wikimedia.org/w/index.php?curid=21897854">Link</a></p>
</center>

<p>So, <code class="highlighter-rouge">D[:, 0:2].transpose()</code> is a matrix of shape \(n \times m\), and is our correct data input matrix \(D_x\). We save this matrix to the variable <code class="highlighter-rouge">X_data</code>. Likewise, we also transpose <code class="highlighter-rouge">D[:, 2]</code> to correctly compute \(D_y\), and save it in <code class="highlighter-rouge">y_data</code>.</p>

<p>At this point we have our \(m \times n\) input data matrix <code class="highlighter-rouge">X_data</code> and our \(m \times 1\) output vector <code class="highlighter-rouge">y_data</code> loaded. In addition, we conveniently have the number of columns stored in <code class="highlighter-rouge">n</code>, so now we can start defining our model.</p>

<h3 id="defining-the-model">Defining the model</h3>

<p>As shown above, we want our model parameters to consist of a matrix \(A\) of size \(1 \times n\) and a single number \(b\). Then, we define:
\[
    y’(x, A, b) = Ax + b
\]</p>

<p>First, we can define the input and correct output placeholders:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define data placeholders</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
</code></pre></div></div>

<p>And then we can define the trainable variables, the output prediction, and the loss function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define trainable variables</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"A"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"b"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>

<span class="c"># Define model output</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="c"># Define the loss function</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="training-the-model">Training the model</h3>

<p>At this point, we have a 1 dimensional output <code class="highlighter-rouge">y_predicted</code> which we compare against <code class="highlighter-rouge">y</code> using <code class="highlighter-rouge">L</code> to train the model, which is exactly the same situation as single variable linear regression. The remaining code to train the model is extremely similar, so I’ll simply display it here, and then explain the few differences:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define optimizer object</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c"># Create a session and initialize variables</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c"># Main optimization loop</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">,</span> <span class="n">current_A</span><span class="p">,</span> <span class="n">current_b</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">X_data</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">y_data</span>
    <span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"t = </span><span class="si">%</span><span class="s">g, loss = </span><span class="si">%</span><span class="s">g, A = </span><span class="si">%</span><span class="s">s, b = </span><span class="si">%</span><span class="s">g"</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">current_A</span><span class="p">),</span> <span class="n">current_b</span><span class="p">))</span>
</code></pre></div></div>

<p>First, we have a different learning rate than the learning rate used in single variable regression. Even though the training algorithm is the same, since this is a different problem than single variable regression, we need find a good learning rate specific to this problem. A great way to do this for your own problems is using TensorBoard, as explained in the chapter <a href="https://donaldpinckney.com/books/tensorflow/book/ch2-linreg/2017-12-27-optimization.html">Optimization Convergence</a>.</p>

<p>Besides this, the only other conceptual difference is that at each step of the optimizer we are modifying the entire vector <code class="highlighter-rouge">A</code> (in addition to <code class="highlighter-rouge">b</code>), rather than just a single number. However, TensorFlow abstracts this away for us, and conceptually we just need to know that we are training the variable <code class="highlighter-rouge">A</code>.</p>

<p>The final print statements should output something close to:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t = 1994, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1995, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1996, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1997, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1998, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1999, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
</code></pre></div></div>

<p>At this point we have converged to our approximate solution of \(A \approx \begin{bmatrix}
            2.005, 
            1.302 
    \end{bmatrix}, b \approx 3.95\). Note that this is not exactly the same as the expected answer of \(A = \begin{bmatrix}
            2, 
            1.3 
    \end{bmatrix}, b \approx 4\), primarily because some random noise was added to each point in the data set.</p>

<p>The model is fully trained, so now given a new input \(x\) we could now predict the output \(y’ = Ax + b\), using all the learned information from all input variables.</p>

<h1 id="concluding-remarks">Concluding Remarks</h1>

<p>Linear regression with multiple variables is only slightly different in essence from single variable linear regression. The main difference is abstracting the linear operation \(ax\) where \(a\) and \(x\) are single numbers to the linear operation \(Ax\), where now \(A\) is a matrix, \(x\) is a vector. In addition, at the implementation level we also have to deal with loading data in a more sophisticated manner, but otherwise the code is mostly the same. In later chapters we will use this abstraction we have built to define even more powerful models.</p>

<h1 id="exercises">Exercises</h1>

<p>So far this chapter has used a synthetic data set, <code class="highlighter-rouge">linreg-multi-synthetic-2.csv</code>, for easy demonstration. The exercises are primarily concerned with getting practice at applying this model to real-world data. Note that in real-world data not all columns are useful, and some might not have a linear relationship with the MPG. Including these unhelpful columns in your model might decrease the accuracy of your model. You should try plotting various columns vs. the output column to determine which seem most helpful in predicting the output, and then only include these useful columns as your input.</p>

<p>In addition, many data sets will have so called <em>messy data</em>, which require you to do some manipulation in Python to make sure the data is imported cleanly and properly. For example, some rows might containg missing data: for these your code can not crash or incorrectly import the data. Instead, you need to adopt a strategy to still import the data as best as you can: for example, you can simply ignore any rows that have incomplete data.</p>

<p>Note that we have not discussed how to rigorously evaluate how good a model is yet. For now you can use the value of the loss function, along with some intuition and creating plots. Evaluation will be discussed more in chapter 2.7.</p>

<ol>
  <li>Download <a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">this red wine quality data set</a>, and try to predict the quality of the wine (last column) from the physicochemical input data (other columns).</li>
  <li>Download <a href="https://archive.ics.uci.edu/ml/datasets/Auto+MPG">this car MPG data set</a>, and try to predict the MPG (first column) based on some of the other columns.</li>
  <li>Download <a href="https://www.kaggle.com/camnugent/california-housing-prices">this California 1990 Housing Value data set</a>, and try to predict the house values based on various factors.</li>
</ol>

<h1 id="complete-code">Complete Code</h1>

<p>The <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/multi_var_reg.py">complete example code is available on GitHub</a>, as well as directly here:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c"># First we load the entire CSV file into an m x 3</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"linreg-multi-synthetic-2.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c"># We extract all rows and the first 2 columns into X_data</span>
<span class="c"># Then we flip it</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="c"># We extract all rows and the last column into y_data</span>
<span class="c"># Then we flip it</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="c"># And make a convenient variable to remember the number of input columns</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c"># Define data placeholders</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>

<span class="c"># Define trainable variables</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"A"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"b"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>

<span class="c"># Define model output</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="c"># Define the loss function</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c"># Define optimizer object</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c"># Create a session and initialize variables</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c"># Main optimization loop</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">,</span> <span class="n">current_A</span><span class="p">,</span> <span class="n">current_b</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">X_data</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">y_data</span>
    <span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"t = </span><span class="si">%</span><span class="s">g, loss = </span><span class="si">%</span><span class="s">g, A = </span><span class="si">%</span><span class="s">s, b = </span><span class="si">%</span><span class="s">g"</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">current_A</span><span class="p">),</span> <span class="n">current_b</span><span class="p">))</span>

</code></pre></div></div>

</div>



<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/tensorflow/2017/12/27/optimization.html" class="titular">
            Exploring Optimization Convergence
            <small>27 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/tensorflow/2017/12/03/single-variable.html" class="titular">
            Single Variable Linear Regression
            <small>03 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/06/06/content-coming-soon.html" class="titular">
            A Sneak Peek of What's to Come
            <small>06 Jun 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
