<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- <link href="https://gmpg.org/xfn/11" rel="profile"> -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/fonts/abril_pt.css">
  <!-- <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- CMU Fonts -->
  <style type="text/css">
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunss.otf');
    }
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunsx.otf');
      font-weight: bold;
    }
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunsi.otf');
      font-style: italic, oblique;
    }
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunbxo.otf');
      font-weight: bold;
      font-style: italic, oblique;
    }
    @font-face {
      font-family: "Classical Computer Modern";
      src: url('/public/fonts/cmunci.otf');
      /* font-style: italic, oblique; */
    }
  </style>

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <!-- MathJax -->
  <!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script> -->
  <!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML'></script> -->
  <!-- <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
  </script> -->

  <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- jQuery -->
  <script src="/public/js/jquery.js"></script>

  <!-- TOC -->
  <script src="/public/js/toc.js"></script>

  <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Topological Data Analysis | Donald Pinckney</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Topological Data Analysis" />
<meta name="author" content="Donald Pinckney" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1st year programming languages PhD student at UMass Amherst." />
<meta property="og:description" content="1st year programming languages PhD student at UMass Amherst." />
<link rel="canonical" href="https://donaldpinckney.com/machine%20learning/2019/04/27/tda.html" />
<meta property="og:url" content="https://donaldpinckney.com/machine%20learning/2019/04/27/tda.html" />
<meta property="og:site_name" content="Donald Pinckney" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-27T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"1st year programming languages PhD student at UMass Amherst.","author":{"@type":"Person","name":"Donald Pinckney"},"@type":"BlogPosting","url":"https://donaldpinckney.com/machine%20learning/2019/04/27/tda.html","headline":"Topological Data Analysis","dateModified":"2019-04-27T00:00:00-07:00","datePublished":"2019-04-27T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://donaldpinckney.com/machine%20learning/2019/04/27/tda.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body class="theme-base-0b">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Donald Pinckney
        </a>
      </h1>
      <p class="lead">1st year programming languages PhD student at UMass Amherst.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="horizontal-block">
        <div class="horizontal-item">
        </div>
        
        

        <a class="horizontal-item sidebar-nav-item" href="/">Home</a>

        

        
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
              
              <a class="horizontal-item sidebar-nav-item " href="/publications.html">Publications</a>
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
        
          
        

        <a class="horizontal-item sidebar-nav-item active" href="/blog/">Blog Posts</a>

        <a class="horizontal-item sidebar-nav-item" href="/books/tensorflow/book/">TensorFlow Guide</a>

        <div class="horizontal-item">
        </div>
      </div>

      <hr />

      <div class="horizontal-block">
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        


        <a href="https://github.com/donald-pinckney" class="sidebar-nav-item horizontal-item social" target="_blank">
          <i class="fa fa-github"></i><span class="only-desktop"> donald-pinckney</span>
        </a>
        <a href="https://twitter.com/donald_pinckney" class="sidebar-nav-item horizontal-item social" target="_blank">
          <i class="fa fa-twitter"></i><span class="only-desktop">@donald_pinckney</span>
        </a>
        <a href="mailto:dpinckney@cs.umass.edu" class="sidebar-nav-item horizontal-item social" target="_blank">
          <i class="fa fa-envelope"></i>&nbsp;<span class="only-desktop">dpinckney@cs.umass.edu</span>
        </a>
        <a href="/public/files/documents/resume.pdf" class="sidebar-nav-item horizontal-item social" target="_blank">
            <i class="fa fa-file"></i>&nbsp;<span>Curriculum vitae</span>
        </a>

      </div>
    </nav>

    <div class="sidebar-footnote">
      <p class="sidebar-footnote">
        &copy; 2019 Donald Pinckney. All rights reserved.
        <!-- <br />
        The views and opinions expressed here are my own, and do not reflect the views of Apple, Inc. -->
      </p>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">

  
    <h1 class="post-title titular">Topological Data Analysis</h1>
  
  
    <div class="floating-box-right">
      


      
    </div> 

  <span class="post-date">27 Apr 2019. Categories: 
  <!-- <div class="post-categories"> -->
  
  
  <a href="/categories/#Machine Learning">Machine Learning</a>
  
  
<!-- </div> -->

</span>



<div id="toc"></div>
<script type="text/javascript">
  $(document).ready(function() {
    // alert("READY");
      $('#toc').toc();
  });
</script>


<script src="/public/post_assets/tda/d3.min.js"></script>

<script src="/public/post_assets/tda/tsne.js"></script>

<script src="/public/post_assets/tda/demo-configs.js"></script>

<!-- <script src="figure-configs.js"></script> -->
<script src="/public/post_assets/tda/visualize.js"></script>

<script src="/public/post_assets/tda/complexes.js"></script>

<script src="/public/post_assets/tda/figures.js"></script>

<link href="/public/post_assets/tda/material-icons.css" rel="stylesheet" />

<link rel="stylesheet" href="/public/post_assets/tda/playground.css" />

<link rel="stylesheet" href="/public/post_assets/tda/post.css" />

<p>The overall goal of Topological Data Analysis (TDA) is to be able to analyze topological features of data sets, often through computations of topological properties such as homology or via visualization. Here I will focus on the former technique, known as <strong>persistent homology</strong>, but I will briefly touch on the visualization aspect. Before jumping into the mathematical aspects I’ll first give an overview of some motivations for TDA by both offering it as an alternative to typical statistical tools as well as showing some of the unique capabilities of TDA.</p>

<div>
\(
   \def\R{\mathbb R}
   \def\N{\mathbb N}
   \def\X{\mathbb X}
   \def\homt{\simeq}
   \def\C{\check{C}}
   \def\e{\varepsilon}
   \def\P{\mathcal{P}}
   \require{AMScd}
   \require{extpfeil}
   \Newextarrow{\vxmapsto}{5,10}{0x21A7}
   \require{HTML}
   \DeclareMathOperator{\Ima}{Im}
\)
</div>

<h1 id="motivations-for-understanding-topological-properties-of-data">Motivations for Understanding Topological Properties of Data</h1>

<p>In many sciences and other fields involving data collection and analysis, one can often consider two types of data analysis: qualitative and quantitative. Given a large and potentially complex data set \(D\), the task of qualitative data analysis is to investigate if \(D\) contains information you are looking for, and this is often performed via some type of visualization. If you then feel that \(D\) does contain desired information, then you can often proceed to applying a more quantitative technique for extracting that information, guided by your intuition from the qualitative analysis. I claim that common statistical techniques are very useful for both tasks, but on more complex data sets currently emerging in today’s sciences can be inadequate, and that a topological approach may help. Let’s look at some specific examples of both qualitative and quantitative data analysis, and motivate why topological techniques might help.</p>

<h2 id="clustering-and-dimensionality-reduction">Clustering and Dimensionality Reduction</h2>

<p>Consider an experiment in which we have a population consisting of multiple bacteria species<sup id="fnref:bio"><a href="#fn:bio" class="footnote">1</a></sup>, and we would like to determine how many different species are present. To do so we can use a technique called <a href="https://en.wikipedia.org/wiki/Raman_spectroscopy">Raman spectroscopy</a>, which in summary shines a laser into your sample and records a emission spectra of the sample. In this paper 4 species with 1000 bacteria each were tested with Raman spectroscopy (giving 4000 spectra) with each spectra consisting of 1500 intensity measurements for wavelengths between 500 and 3500 \(cm^{-1}\). Mathematically this gives us 4000 points living in \(\R^{1500}\). Now we want to see if the spectra can actually be used to differentiate the 4 bacteria species. Unfortunately, visualization of \(\R^{1500}\) directly is not helpful, as the following plot of overlapping spectra shows:</p>

<p><img src="/public/post_assets/tda/spectra.png" alt="raman spectra" class="center" /></p>

<p>A very standard technique for reducing this high dimensional data down to a smaller dimension is <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> (Principle Components Analysis) which finds the linear projection which maximizes the variance. Using PCA to project \(\R^{1500}\) to \(\R^2\) gives the following plot, with colors indicating the previously known species:</p>

<p><img src="/public/post_assets/tda/raman_pca.png" alt="raman pca" class="center" /></p>

<p>PCA has provided some useful information, but not so much as we would like. This linear projection shows that some differentiation of species using the spectra is possible, but unfortunately PCA can not be used to differentiate the blue and light blue samples as they strongly overlap. Any typical algorithm used to identify clusters (such as <a href="https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm">k-means clustering</a>) would fail to isolate 4 clusters. The original data in \(\R^{1500}\) probably contains the information necessary to differentiate all 4 species, but we can’t visualize it well directly. On the other hand linearly projecting to a low dimension destroys the useful information we want. Projecting to a higher dimensional space like \(\R^3\) or using non-linear dimensionality reductions such as t-SNE <sup id="fnref:tsne"><a href="#fn:tsne" class="footnote">2</a></sup> or diffusion maps <sup id="fnref:diff"><a href="#fn:diff" class="footnote">3</a></sup> might or might not work better, but at the end of the day can run into the same limitations. Using topology we can take a different approach: <em>imagine we can regard the data as a manifold \(X \subseteq \R^{1500}\)</em>; then detecting the number of different species (clusters) corresponds to determining the number of connected components of \(X\). Using TDA techniques we can determine the number of connected components in the original data set, <em>without having to project to lower-dimensions</em>. This type of clustering analysis is applicable to a huge number of problems, not just in the biological domain.</p>

<h2 id="understanding-higher-dimensional-topological-features-of-primary-visual-cortex-of-monkeys">Understanding Higher-Dimensional Topological Features of Primary Visual Cortex of Monkeys</h2>

<p>However, TDA can be used for more than just inspecting connected components. One particularly interesting application of TDA to neuroscience is discussed in section 2.5 of Carlsson <sup id="fnref:car"><a href="#fn:car" class="footnote">4</a></sup>. In an experiment a 10x10 array of electrodes were implanted in the Primary Visual Cortex of Macaque monkeys, while the monkeys viewed either a blank screen or some movie clips. A bunch of signal processing techniques were applied to the voltage sequences, eventually resulting in a collection of data sets, each consisting of 200 data points lying in \(\R^5\). Each such data set \(D_i \subseteq \R^5\) corresponds to the monkey looking at either a blank screen or movie clips, for a 10 second window. Each point \(p \in D_i \subseteq \R^5\) are the 5 voltages during a 50ms window of the top 5 activated neurons in the 10 second window associated to \(D_i\). So we have many such \(D_i \subseteq \R^5\), of which some correspond to watching blank screens and others correspond to watching movie clips.</p>

<p>The task now is to differentiate the \(D_i\)’s based on blank screen vs. movie clips. Given some \(D_i\), <em>if we can view \(D_i\) as a topological space</em>, then we can apply the algebraic topology tool of homology to compute the Betti numbers of \(D_i\). In this study the first 3 Betti numbers \((\beta_0, \beta_1, \beta_2)\) were computed for each \(D_i\). By a vast margin the most common Betti numbers were \(a = (1, 1, 0)\) and \(b = (1, 0, 1)\), corresponding to the topology of a circle and a sphere. Finally Betti numbers were computed for data sets generated by random firings according a Poisson model (i.e. the null hypothesis). The distribution of Betti numbers is easily able to distinguish all three modes (blank screen, movie clips, random Poisson model) from each other. In addition since \(a\) and \(b\) were the most common Betti numbers for both blank screen and movie clip modes, this suggests that the primary visual cortex seems to naturally operate using the topology of circles and spheres, but the reason for this topological phenomenon is not yet known.</p>

<h1 id="point-clouds-and-simplicial-complexes">Point Clouds and Simplicial Complexes</h1>

<p>In both of the examples above and in nearly all data analysis settings one has a discrete finite data set \(D\). Often \(D \subseteq \R^n\), but more generally we consider \(D\) to be some discrete finite metric space (for example in comparing DNA sequences one can define a distance metric between sequences which is non-Euclidean). \(D\) is often called a <strong>point cloud</strong>. One way to look at this is that the point cloud \(D\) is drawn from a probability distribution with support some metric space \(X\). For example in the bacteria species classification experiment \(X\) would be the space of possible emission spectra for the 4 different species, and we would expect \(X\) to have 4 connected components. Thus the goal of TDA is to infer the topological properties of \(X\) given a point cloud \(D\) sampled from \(X\).</p>

<p>Since \(D\) is a discrete space, it is useless topologically as is. The main strategy of TDA is to build a simplicial complex from \(D\) such that the topological properties of the simplicial complex are similar to those of \(X\). Of course this isn’t always possible as it is contingent upon a sufficient number of samples.</p>

<h2 id="nerves-of-open-covers-and-the-Čech-complex">Nerves of Open Covers and the Čech Complex</h2>

<p>The first and most natural technique for building a simplicial complex based on some topological space \(X\) is based on an open cover \(\mathcal{U}\) of \(X\). If \(\mathcal{U} = \{U_i\}, i \in I\) is an open cover of \(X\) then the simplicial complex called the <strong>nerve of \(\mathcal{U}\)</strong>, \(N(\mathcal{U})\), is defined to have vertex set \(I\), with a \(k-1\) simplex \(\{i_1, \cdots, i_k\} \in N(\mathcal{U})\) if and only if:
\[
   U_{i_1} \cap U_{i_2} \cap \cdots \cap U_{i_k} \neq \emptyset
\]</p>

<p>The following picture from Wikipedia illustrates this well:</p>

<p><a title="ProboscideaRubber15 [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Constructing_nerve.png" class="center"><img width="512" alt="Constructing nerve" src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Constructing_nerve.png/512px-Constructing_nerve.png" /></a></p>

<p>Note that if at most \(n\) open sets intersect simultaneously then \(N(\mathcal{U})\) has dimension \(n-1\). A basic result of nerves is the Nerve Lemma:</p>
<div class="lemma" text="Nerve Lemma">
   Let \(X\) be a topological space, and let \(\mathcal{U}\) be an open cover of \(X\). If every non-empty intersection of open sets in \(\mathcal{U}\) is contractible, then \(N(\mathcal{U}) \homt X\).
</div>

<p>We can now define the <strong>Čech Complex</strong> of a finite point cloud \(D \subseteq X\). Let \(\varepsilon &gt; 0\), and define the open cover \(\mathcal{U} = \{ B_\varepsilon (x) \mid x \in D \} \).  Then the Čech Complex of radius \(\varepsilon\) is defined as \(\check{C}_\varepsilon(D) = N(\mathcal{U})\).</p>

<h2 id="homotopy-type-of-the-Čech-complex">Homotopy Type of the Čech Complex</h2>

<p>To better understand the topological properties of the Čech Complex we can apply the Nerve Lemma. By the Nerve Lemma, \(\check{C}_\varepsilon(D) \homt \bigcup_{x \in D} B_\varepsilon(x)\). However, what we would like is to relate the topology of \(\C_\e(D)\) to the topology of \(X\), where \(D\) has been sampled from \(X\). The following theorem<sup id="fnref:car:1"><a href="#fn:car" class="footnote">4</a></sup> is a partial result in this direction:</p>
<div class="theorem">
   Let \(X\) be a Riemannian manifold. Then there exists an \(e\) such that for all \(\e \leq e\), there is a finite point cloud \(D \subseteq X\) such that \(\C_\e(D) \homt X\).
</div>
<p>This theorem is a nice theoretic result, since it hints that Čech Complexes of point clouds are morally the correct objects to study as they do have a topological relationship with the underlying topological space. However in terms of practicalities this theorem buys us little. The problems are the following:</p>

<ol>
  <li>Since we only have access to \(D\) and not \(X\), how can we know that \(X\) is a Riemannian manifold?</li>
  <li>The existence of such an \(e\) is good, but the theorem offers no way to find or search for \(e\).</li>
  <li>The largest issue is that we only have one given point cloud \(D\), so the existence of another \(D’\) guaranteeing homotopy equivalence is not useful.</li>
</ol>

<p>There have been further results that partially address some of these issues, of which Chazal<sup id="fnref:chazal"><a href="#fn:chazal" class="footnote">5</a></sup> surveys a few. An additional problem with Čech Complexes is that computationally they are quite slow to compute, which leads us to consider alternate simplicial complex constructions such as the <a href="https://en.wikipedia.org/wiki/Vietoris–Rips_complex">Vietoris-Rips Complex</a> and many others which are surveyed by Carlsson<sup id="fnref:car:2"><a href="#fn:car" class="footnote">4</a></sup>. However, the simplicial complex constructions are all similar in that they all attempt to reconstruct the homotopy type of \(X\) and can do so under strict assumptions, but in practical data analysis situations will almost always fail to be homotopy equivalent.</p>

<h2 id="exploring-some-Čech-complexes">Exploring Some Čech Complexes</h2>

<p>To get a better intuition for why Čech Complexes of point clouds \(D\) sampled from a topological space \(X\) fail to be homotopy equivalent to \(X\), we can look at visualizations of Čech Complexes on sample data sets. Since Čech Complexes can be quite high dimensional, we only visualize the 2-skeleton of the Čech Complex. Below you can interact with a variety of sample data sets, and vary the parameter \(\varepsilon\) to see how it affects \(\C_\e(D)^{(2)}\).</p>

<div id="playground"><div id="playground-canvas"></div><div id="data-menu"></div><div id="data-details"><div id="data-controls"><div id="data-options"></div><div id="tsne-options"></div></div><div id="data-description"><span></span></div></div></div>
<script src="/public/post_assets/tda/playground.js"></script>

<p>The above code was based on Wattenberg, et al.<sup id="fnref:distill"><a href="#fn:distill" class="footnote">6</a></sup>. Play around with the Čech Complex visualization as long as you want. But once you’re done, here are some of the key takeaways from it.</p>

<p>First, due to random sampling noise and non-homogenous density it can be tricky to chose a value of \(\e\) such that \(\C_\e(D) \homt X\). For example with the annulus you have to pick some \(\e\) that is large enough to fill in some holes from noise, but if you pick an \(\e\) that is way too big then it will fill in the hole of the annulus itself. For these toy data sets we can have an intuition for what choice of \(\e\) “looks right”, but for high dimensional data it is hard to have an understanding of a “good” choice of \(\e\).</p>

<p>Second, there can be multiple “good” choices of \(\e\). For example, consider the 4th data set above with 2 circles of vastly different sizes. For \(\e \approx 0.07\) The tiny circle obtains the correct topology, but the big circle is still disconnected. But if you try to increase \(\e\) so as to make the big circle connected, the tiny circle actually becomes simply connected. Both the big and tiny circles have the homotopy type of a circle, but the homotopy type of each is only visible at distinct values of \(\e\). In a sense \(\e\) is like the amount of zoom in a microscope: you can make \(\e\) very small to zoom in closely and inspect the topology of the tiny circle, or you can make \(\e\) large to zoom out and observe the topology of the big circle, at which point the tiny circle is negligible.</p>

<p>The second point really provides the key insight of TDA: <em>There is no one best value of \(\e\), but rather the spectrum of topologies as \(\e\) varies provides a comprehensive view of the topology of the point cloud.</em> This will also address the first point, as the holes due to noisy sampling only last for a short range of \(\e\) values. Thus, in TDA we study families of simplicial complexes such as \(\P = \{\C_\e(D) \mid \e \in T \subseteq \R \}\). The crucial property of these Čech Complex families is that for \(\e \leq \e’\) we have that \(\C_\e(D)\) is a subcomplex of \(\C_{\e’}(D)\) (this is a trivial result of the definition of a Čech Complex). Also note that since point clouds are finite, only finitely many \(\e\) will produce distinct Čech Complexes, so we can consider \(T\) to be finite. Other simplicial complex constructions such as Vietoris-Rips complexes also share these properties.</p>

<h1 id="persistence">Persistence</h1>

<h2 id="persistence-objects">Persistence Objects</h2>

<p>We briefly set aside the specific construction of Čech Complexes and work from an algebraic perspective. Let \((T, \leq)\) be any partially ordered set and let \(\underline{C}\) be any category. We say that a <strong>\(T\)-persistence object</strong> is a family \( \{c_t\}_{t \in T} \subseteq \underline{C} \) together with a morphism \(\phi^{t,u} : c_t \to c_u \) whenever \(t \leq u\) such that if \(t \leq u \leq v\) then \(\phi^{u,v} \circ \phi^{t,u} = \phi^{t,v}\). Alternatively a \(T\)-persistence object can be viewed as a functor \(\Phi : T \to \underline{C}\). Diagrammatically this looks something like:</p>

<div>
\[
\begin{array}{ccc}
t &amp; \xmapsto{\Phi} &amp; c_t \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{t,u} \\
u &amp; \xmapsto{\Phi} &amp; c_u \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{u,v} \\
v &amp; \xmapsto{\Phi} &amp; c_v 
\end{array}
\]
</div>

<p>Suppose we have a \(T\)-persistence object \( \{c_t, \phi^{t,u} \} \) and some partial order preserving map \(f : T’ \to T\) for some other partially ordered set \(T’\). Then we have an induced \(T’\)-persistence object \( \{c_{f(t’)} \}_{t’ \in T’} \) with morphisms \(\psi^{t’,u’} : c_{f(t’)} \to c_{f(u’)}\) given by \(\psi^{t’,u’} = \phi^{f(t’),f(u’)}\). It sounds more complicated than it is, so hopefully a diagram clarifies:</p>

<div>
\[
\begin{array}{ccccc}
t' &amp; \xmapsto{f} &amp; t &amp; \xmapsto{\Phi} &amp; c_t \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{t,u} \\
u' &amp; \xmapsto{f} &amp; u &amp; \xmapsto{\Phi} &amp; c_u \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{u,v} \\
v' &amp; \xmapsto{f} &amp; v &amp; \xmapsto{\Phi} &amp; c_v 
\end{array}
\]
</div>

<h2 id="Čech-complex-families-as-persistence-objects">Čech Complex families as Persistence Objects</h2>

<p>We can easily see that Čech Complex families are \(\R\)-persistence simplicial complexes since for all \(\e \leq \e’\) we have the inclusion map \(i^{\e,\e’}: \C_\e(D) \hookrightarrow \C_{\e’}(D)\) as discussed above (the superscript notation denotes Čech Complex radius, and is unrelated to cohomology). In addition since there are only finitely many values of \(\e\), say \(E = \{\e_1, \cdots, \e_n\} \subseteq \R\), which produce distinct Čech Complexes, we can easily construct an order preserving map \(f: \N \to E\), and thus we have that a Čech Complex family is an \(\N\)-persistence simplicial complex. For now we will work with \(\R\)-persistence, but will return to using \(\N\)-persistence later.</p>

<p>The inclusion maps \(i^{\e,\e’}: \C_\e(D) \hookrightarrow \C_{\e’}(D)\) for an \(\R\)-persistence simplicial complex then induce chain maps for the simplicial chain complex \( i_k^{\e,\e’}: C_k(\C_\e(D); G) \rightarrow C_k(\C_{\e’}(D); G) \), and likewise for the simplicial homology \( i_k^{\e,\e’}: H_k(\C_\e(D); G) \rightarrow H_k(\C_{\e’}(D); G) \) for any coefficient group \(G\).  We can see this in a diagram, for \(\e \leq \e+p\):</p>

<!-- Thus, we have that the homology groups \\(H_\*(\C_\e(D); R)\\) are an \\(\R\\)-persistence \\(R\\)-module. -->

<div>
\[
\begin{array}{ccccc}
                   &amp; \vdots \;\;\;\;\;\;  &amp;                          &amp; \vdots \;\;\;\;\;\;      &amp; \\
                   &amp; \big\downarrow \;\;\;\;\;\;   &amp;                          &amp; \big\downarrow \;\;\;\;\;\;      &amp; \\
\cdots \rightarrow &amp; H_k(\C_\e(D); G) &amp; \xrightarrow{\partial_*} &amp; H_{k-1}(\C_\e(D); G) &amp; \rightarrow \cdots \\
                   &amp; \big\downarrow \; i_k^{\e,\e+p}   &amp;                          &amp; \big\downarrow \; i_{k-1}^{\e,\e+p}       &amp; \\
\cdots \rightarrow &amp; H_k(\C_{\e+p}(D); G) &amp; \xrightarrow{\partial_*} &amp; H_{k-1}(\C_{\e+p}(D); G) &amp; \rightarrow \cdots \\
                   &amp; \big\downarrow \;\;\;\;\;\;   &amp;                          &amp; \big\downarrow \;\;\;\;\;\;      &amp; \\
                   &amp; \vdots \;\;\;\;\;\;  &amp;                          &amp; \vdots \;\;\;\;\;\;      &amp; 
\end{array}
\]
</div>

<p>To better understand what these inclusion induced maps are doing, we can look at a specific example from the visualization above. Consider two choices of \(\e\) for the annulus:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">\(\C_\e(D)\)</th>
      <th style="text-align: center">\(\C_{\e+p}(D)\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/public/post_assets/tda/annulus1.png" alt="annulus1" /></td>
      <td style="text-align: center"><img src="/public/post_assets/tda/annulus2.png" alt="annulus2" /></td>
    </tr>
  </tbody>
</table>

<p>In the left Čech Complex there are 3 generators for \(H_1\), shown in red, purple and orange. If we slightly increase \(\e\) to \(\e+p\) in the right Čech Complex we kill the purple and orange generators, but a new green generator is born. But if we look at the image of the homomorphism induced by the inclusion we see only that purple and orange are killed, leaving only the red generator in the image. In this case the inclusion induced homomorphism for \(\e\) to \(\e+p\) killed the topological features introduced by sampling noise, but whether or not this happens depends on the values of \(\e\) and \(p\). We define a new object called the <strong>\(p\)-persistent homology</strong> in order to capture the elements of \(H_k\) that survive an increase of the Čech Complex radius by \(p\). Formally, we define the level-\(k\) \(p\)-persistent homology group starting at radius \(\e\) to be:
\[
   H_k^{\e,\e+p}(\C_\e(D); G) := \Ima (\;i^{\e,\e+p} : H_k(\C_\e(D); G) \to H_k(\C_{\e+p}(D); G)\;)
\]
This is precisely the elements of the level-\(k\) homology of the Čech Complex with radius \(\e\) that are not killed by increasing the radius by \(p\). Note that this generalizes normal homology, as \(H_k^{\e,\e+0}(\C_\e(D); G) = H_k(\C_\e(D); G)\).</p>

<h2 id="graded-modules-correspondence">Graded Modules Correspondence</h2>

<p>The \(p\)-persistent homology groups are what we would like to compute, but unfortunately the definition above does not offer a scheme for carrying out the computation. To remedy this we will develop a structure theorem for persistence. Let \(\mathcal{M}\) be an \(\N\)-persistence module over \(R\). Concretely we have a family of \(R\)-modules \(\{M^i\}_{i \in \N}\) with linear maps \(\phi^i : M^i \to M^{i+1}\) (this is sufficient to generate maps from \(M^i\) to \(M^j\) for \(i \leq j\)). Let \(R[t]\) with degree of \(t\) being 1 be the standard graded polynomial ring. We map \(\mathcal{M}\) to a graded module over \(R[t]\) as follows:</p>

<p>\[
   \alpha(\mathcal{M}) := \bigoplus_{i=0}^\infty M^i
\]</p>

<p>We say that the \(n\)-th graded part of \(\alpha(\mathcal{M})\) is \(M^i\). Lastly we need to describe scalar multiplication by the polynomial generator \(t\) on a vector \((m^0, m^1, m^2, \cdots)\) for \(m^i \in M^i\). We define this as:
\[
   t (m^0, m^1, m^2, \cdots) := (0, \phi^0(m^0), \phi^1(m^1), \phi^2(m^2), \cdots)
\]
The action of \(t\) is simply to shift each element up one gradation level by means of using the linear maps \(\phi^i\) provided by the persistence module. It is easy to check that \(\alpha\) defines a functor from the category of persistence modules over \(R\) to the category of graded</p>

<div class="theorem">
   \(\alpha\) defines an equivalence of categories between the category of persistence modules over \(R\) and the category of graded modules over \(R[t]\).
</div>

<div class="proof">
   <p>
   First we show that \(\alpha\) is a functor from the category of persistence modules over \(R\) to the category of graded modules over \(R[t]\). Let \(\mathcal{M}\) and \(\mathcal{N}\) be persistence modules as describe above with linear maps \(\phi^i\) and \(\psi^i\) respectively. Let \(f : \mathcal{M} \to \mathcal{N}\) be a persistence module homomorphism, that is a family of module homomorphisms \(f^i : M^i \to N^i\). From the definition of \(\alpha\) above it is clear that \(\alpha(\mathcal{M})\) is indeed a module with a gradation, but we need to check that the \(i\) gradation of \(R[t]\) multiplied with the \(j\) gradation of the module, \(M^j\), is contained in \(M^{i+j}\). This holds because \(t^i (0, \cdots, 0, m^j, 0, \cdots) = (0, \cdots, 0, (\phi^{i+j-1}\circ\cdots\circ\phi^{j+1}\circ\phi^j)(m^j), 0, \cdots) \in M^{i+j}\). Next we need to check that the persistence module homomorphism \(f\) is mapped to a graded module homomorphism. We define \(\alpha(f) : \alpha(\mathcal{M}) \to \alpha(\mathcal{N})\) to be given by mapping \( (m^0, m^1, \cdots) \mapsto (f^0(m^0), f^1(m^1), \cdots) \). Since each \(f^i\) is a module homomorphism and gradation is respected, \(\alpha(f)\) is a graded module homomorphism. Thus \(\alpha\) is a functor from the category of persistence modules over \(R\) to the category of graded modules over \(R[t]\).
   </p>

   <!-- <p> -->
   Now we show the reverse. Let \(M = \bigoplus_i M^i\) be a graded module over \(R[t]\). We define:
   \[
      \alpha^{-1}(M) := \{M^i\}
   \]
   with the linear map \(\phi^i : M^i \to M^{i+1}\) given by multiplication by \(t\). This is clearly a linear map, and thus \(\alpha^{-1}\) is well-defined. If \(g : M \to N\) is a graded module homomorphism, then \(\alpha^{-1}(g)\) is defined as simply applying \(g\) to each \(M^i\). Clearly \(\alpha^{-1}(g)\) is then a persistence module homomorphism. Finally note that by construction it is trivial to check that \(\alpha\) and \(\alpha^{-1}\) are inverses.
   <!-- </p> -->
</div>
<p>The intuition is that we use multiplication by \(t\) to keep track of the number of times an element of a module is mapped through the linear persistence maps.</p>

<h2 id="classification-theorem-for-mathbbn-persistence-modules">Classification Theorem for \(\mathbb{N}\)-Persistence Modules</h2>

<p>By showing that persistence modules are in correspondence with graded modules over \(R[t]\) opens the door to applying well-know classification theorems of graded modules. Unfortunately there is no simple classification when \(R\) is not a field, but for \(R = F\) a field, there exists a nice result:</p>

<div class="theorem" text="Graded Module Classification">
   If \(M_*\) is a finitely generated graded \(F[t]\) module, then there exist integers \(\{i_1, \cdots, i_m\}\) and \(\{j_1, l_1, \cdots, j_n, l_n\}\) such that:
   \[
      M_* \cong \left( \bigoplus_{s=1}^m t^{i_s} \cdot F[t] \right) \oplus \left( \bigoplus_{r=1}^n t^{j_r} \cdot(F[t] / (t^{l_r} \cdot F[t])) \right)
   \]
   This classification is unique up to permutations of factors.
</div>
<p>There are two components to the classification. On the left side of \(\oplus\), we have the free part which corresponds to elements of the persistence module that appear at index \(i_s\) and never disappear. On the right side we have the torsion part which corresponds to elements of the persistence module that appear at index \(j_r\) and die at index \(l_r\). This classification is the main result we are looking for, since this gives us a concrete computational tool with which to calculate \(p\)-persistence homology groups. In particular the torsion part above corresponds precisely to the \(p\)-persistence homology groups discussed previously. Note that using \(R = F\) a field means that rather than having persistence modules, we have persistence vector spaces. But to be able to apply this theorem we need to know that our graded modules over \(F[t]\) are finitely generated. The following theorem provides an exact condition:</p>

<div class="theorem">
   Let \(\mathcal{V} = \{V^i, \phi^i\}\) be an \(\N\)-persistence vector space over a field \(F\). Then \(\alpha(\mathcal{V})\) (which is a graded module over \(F[t]\)) is finitely generated if and only if every \(V^n\) has finite dimension and there exists an \(n\) such that for all \(m \geq n, \phi^m : V^m \cong V^{m+1}\). In other words \(\mathcal{V}\) needs to be finite in both the interior dimension of each vector space and finite in the persistence. Such a \(\N\)-persistence vector space is called <b>tame.</b>
</div>

<p>In light of this condition we can give our final conclusion on the classification of \(\N\)-persistence vector spaces by incorporating the intuition discussed above. For a field \(F\) and any \(0 \leq m \leq n\) we define an \(\N\)-persistence vector space \(\mathcal{U}(m, n) := \{U_i(m, n), \phi^i\}\) where \(U_i(m, n) = F\) for \(m \leq i \leq n\) and 0 otherwise, and \(phi^i\) is defined to be the identity function for \(m \leq i \leq n-1\) and the 0 function otherwise. In addition we allow \(n = \infty\). If \(\mathcal{V}\) is an \(\N\) persistent vector that satisfies the conditions in the above theorem, then we have the following classification result:</p>

<div class="theorem" text="Persistence Vector Space Classification">
   If \(\mathcal{V}\) is a tame \(\N\)-persistence vector space over a field \(F\), then it can be decomposed as:
   \[
      \mathcal{V} \cong \bigoplus_{i=0}^N \mathcal{U}(m_i, n_i)
   \]
   with \(\mathcal{U}(m, n)\) defined as above. This classification is unique up to permutations of factors.
</div>

<p>There is a quite nice and practical way to visualize this decomposition, called a <strong>persistence barcode</strong>. Each \(U(m, n)\) is drawn as a bar from index \(m\) to \(n\) with indices along the x-axis and different bars along the y-axis:</p>

<p><img src="/public/post_assets/tda/barcode.png" alt="sample barcode" class="center" /></p>

<p>The start of a bar at \(m\) corresponds to the birth of a new \(U(m, n)\), until the bar disappears at index \(n\).</p>

<h2 id="applying-persistence-module-theory-to-Čech-complexes">Applying Persistence Module Theory to Čech Complexes</h2>

<p>Let’s leave the area of abstract algebra and conclude by applying the results of persistence module classification to Čech Complexes. Recall that varying values of radii gives inclusion maps between Čech Complexes of increasing radius. These inclusion maps induce homomorphsims at the simplicial chain level and at the simplicial homology level, thus making \(H_k(\C_\e(D); R)\) an \(\R\)-persistence module over \(R\) with linear maps the homomorphisms induced by inclusion. In addition recall that since \(D\) is a finite point cloud only finitely many \(\e\) will produce distinct complexes, so we can enumerate these \(\e_i\)’s to consider \(H_k(\C_\e(D); R)\) an \(\N\)-persistence module over \(R\) with linear maps \(\phi^i = i_k^{\e_i,\e_{i+1}}\).</p>

<p>In order to apply the classification results we must choose \(R = F\) to be a field, such as \(\mathbb{Z}_2\), and then we have \(H_k(\C_\e(D); R)\) is an \(\N\)-persistence vector space over \(F\). In addition since \(D\) is finite each Čech Complex built from \(D\) will be finite and thus each \(H_k(\C_\e(D); F)\) will be finitely generated. Lastly, since we have only finitely many \(\e\), \(H_k(\C_\e(D); F)\) is tame. Therefore we can use the classification theorem above to compute persistent homology barcodes!</p>

<h1 id="using-the-gudhi-python-library-to-compute-persistent-homology">Using the GUDHI Python Library to Compute Persistent Homology</h1>

<p>I would like to conclude on a matter of practicality. Algorithms to compute persistent homology have been <a href="http://gudhi.gforge.inria.fr">implemented for Python in the GUDHI Library</a>. Using computing persistent homology via GUDHI is quite easy, and I believe can offer interesting and new insights for fields with data analysis. I won’t include a full tutorial, but a simple example of GUDHI in action should be sufficient to show how easy it is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gudhi</span> <span class="k">as</span> <span class="n">gd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">circle_sample</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">),</span> <span class="n">r</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>

<span class="c"># First we sample 150 points from the unit circle and the radius 2 circle</span>
<span class="n">xs1</span><span class="p">,</span> <span class="n">ys1</span> <span class="o">=</span> <span class="n">circle_sample</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">xs2</span><span class="p">,</span> <span class="n">ys2</span> <span class="o">=</span> <span class="n">circle_sample</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">xs1</span><span class="p">,</span> <span class="n">xs2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">ys1</span><span class="p">,</span> <span class="n">ys2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c"># Then we build a Vietoris-Rips complex (2 skeleton)</span>
<span class="n">pts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">))</span>
<span class="n">rc</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">RipsComplex</span><span class="p">(</span><span class="n">points</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">max_edge_length</span><span class="o">=</span><span class="mf">4.1</span><span class="p">)</span>
<span class="c"># Then we build a simplex tree, which is a fast data structure encoding the simplex</span>
<span class="c"># This is just an intermediate step</span>
<span class="n">tr</span> <span class="o">=</span> <span class="n">rc</span><span class="o">.</span><span class="n">create_simplex_tree</span><span class="p">(</span><span class="n">max_dimension</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># Finally we compute the persistence, which is a list of the form [(k, (b, d))]</span>
<span class="c"># where k is the homology level, b is the birth time and d is the death time.</span>
<span class="n">diag</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">persistence</span><span class="p">()</span>

<span class="c"># Then we plot the persistence barcode and the original data side by side.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">gd</span><span class="o">.</span><span class="n">plot_persistence_barcode</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">gd</span><span class="o">.</span><span class="n">plot_persistence_diagram</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Original data set"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="s">"ro"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Running this code produces the following plots:</p>

<p><img src="/public/post_assets/tda/circles.png" alt="circles example" class="center-full" /></p>

<p>In particular it shows the correct computation of the Betti numbers \(\beta_0 = 2, \beta_1 = 2\).</p>

<h1 id="references">References</h1>

<!-- Hello!! -->

<div class="footnotes">
  <ol>
    <li id="fn:bio">
      <p><a href="https://www.sciencedirect.com/science/article/pii/S0003267016300137" class="dont-break-out">https://www.sciencedirect.com/science/article/pii/S0003267016300137</a> <a href="#fnref:bio" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tsne">
      <p><a href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" class="dont-break-out">http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</a> <a href="#fnref:tsne" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:diff">
      <p><a href="https://www.sciencedirect.com/science/article/pii/S1063520306000546" class="dont-break-out">https://www.sciencedirect.com/science/article/pii/S1063520306000546</a> <a href="#fnref:diff" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:car">
      <p><a href="http://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf" class="dont-break-out">http://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf</a> <a href="#fnref:car" class="reversefootnote">&#8617;</a> <a href="#fnref:car:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:car:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:chazal">
      <p><a href="https://arxiv.org/abs/1710.04019" class="dont-break-out">https://arxiv.org/abs/1710.04019</a> <a href="#fnref:chazal" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:distill">
      <p>Wattenberg, et al., “How to Use t-SNE Effectively”, Distill, 2016. <a href="http://doi.org/10.23915/distill.00002" class="dont-break-out">http://doi.org/10.23915/distill.00002</a> <a href="#fnref:distill" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>



<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/idris/2019/03/26/idris-serverless.html" class="titular">
            Coding Serverless Functions in Idris
            <small>26 Mar 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/tensorflow/2018/11/15/feature-scaling.html" class="titular">
            Feature Scaling
            <small>15 Nov 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/metal/2018/07/27/metal-intro-2.html" class="titular">
            Metal 3D Graphics Part 2: Animated Uniform Data with Synchronization
            <small>27 Jul 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
